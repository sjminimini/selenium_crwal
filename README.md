# 📸 스크린샷 크롤러

웹 브라우저를 열고, **전체 페이지를 스크린샷으로 저장**하는 Python 크롤러입니다.  
Selenium과 Headless Chrome을 활용하며, `robots.txt` 정책을 사전에 확인하여 윤리적인 크롤링을 수행합니다.

> ✅ 학습 및 리서치 용도로 제작된 프로젝트입니다.

---

## 💡 프로젝트 소개

- 크롤링을 수행할 때 가장 먼저 확인해야 할 요소는 해당 사이트가 **크롤링을 허용하는가**입니다.
- 이 스크립트는 `robots.txt` 정책을 먼저 확인한 뒤, **허용된 경우에만** 크롬 브라우저를 띄워 **전체 페이지 스크린샷을 저장**합니다.
- 이를 통해 정적 웹 페이지의 상태를 기록하고, 추후 웹페이지 내용이 변경되었을 경우 위·변조 여부를 식별할 수 있습니다.

---

## 🚀 기능

- ✅ 크롤링 전 `robots.txt`로 허용 여부 확인
- ✅ Headless Chrome 사용 (브라우저 창 없이 실행)
- ✅ 전체 페이지 높이 캡처 (뷰포트가 아닌 실제 전체 화면)

---

## 🔧 실행 전 확인 사항

- 크롬 브라우저가 설치되어 있어야 합니다.
- ChromeDriver가 시스템 경로에 있어야 하며, 브라우저 버전과 일치해야 합니다.
  - 직접 설치하려면 [ChromeDriver 다운로드 페이지](https://chromedriver.chromium.org/downloads)에서 크롬 버전에 맞는 드라이버를 받아 설치하세요.
  - 설치 후 ChromeDriver 실행 파일 위치를 시스템 PATH에 등록하거나, 
    코드에서 `Service(executable_path="chromedriver 경로")` 형태로 직접 경로를 지정해야 합니다.
- 현재 코드는 `webdriver-manager`를 사용하지 않으므로, 자동 설치 기능은 포함되어 있지 않습니다.
- 자동 관리하는 방식을 원하신다면 아래 명령어를 실행하시면 됩니다.
```bash
pip install selenium webdriver-manager
```

---

## 🛠️ 설치 방법

### 1. Python 패키지 설치

```bash
pip install selenium
```

---

## 📷 사용 방법

1. 스크립트 내 `urls_to_crawl` 리스트에 크롤링할 URL을 추가하거나 수정하세요.
```python
urls_to_crawl = [
    "크롤링 할 url1",
    "크롤링 할 url2",
    "크롤링 할 url3"
]

```
스크린샷은 현재 디렉토리에 자동 저장되며, 파일명은 URL 기반으로 생성됩니다.

## ❗주의사항
- 크롤링은 웹사이트의 규칙(robots.txt) 과 이용 약관을 준수해야 합니다.

- 과도한 요청은 서버에 부하를 줄 수 있으므로 지속적/대량 크롤링은 피해주세요.

- 본 스크립트는 학습용 및 테스트용으로 제작되었습니다.